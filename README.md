HSM-m6Am is a hybrid sequential model designed for the identification of N6,2â€²-O-dimethyl adenosine (m6Am) sites within RNA sequences. Utilizing advanced deep learning (DL) techniques and hybrid feature extraction methods, HSM-m6Am aims to improve the accuracy and efficiency of m6Am site detection compared to existing models.

Key Features
Deep Learning Model: Employs state-of-the-art deep neural networks (DNNs) and convolutional neural networks (CNNs).
Hybrid Feature Extraction: Combines multiple feature extraction techniques for enhanced performance.
Dimensionality Reduction: Uses Shape for efficient feature reduction.
Versatile: Capable of handling large-scale RNA sequence datasets.
System Requirements
To effectively run the HSM-m6Am model, your system must meet specific hardware and software requirements. For hardware, an Intel Core i5 CPU or equivalent is suitable for basic tasks, while an Intel Core i7 or better is recommended for high-performance training. The system should have a minimum of 8 GB of RAM, though 16 GB or more is preferred for handling large datasets. Ensure at least 5 GB of free disk space is available for installation and data storage.

On the software side, the model supports Windows 10 or later, macOS 10.14 or later, or a modern Linux distribution. Python 3.7 or later is required, along with the pip package manager. The necessary libraries, which can be installed using the requirements.txt file, include TensorFlow or PyTorch (depending on your deep learning framework preference), NumPy, Pandas, Scikit-learn, and Matplotlib. Additional dependencies are specified in the requirements.txt file.

For optimal performance, having a compatible GPU is advisable if you plan to utilize GPU acceleration for training deep learning models. If using an NVIDIA GPU, CUDA and cuDNN may be necessary for TensorFlow or PyTorch. Finally, ensure that you have the appropriate permissions to install and run the required software on your system.

The HSM-m6Am model is tuned with specific hyperparameters to achieve optimal performance. It employs a neural network architecture with hidden layers containing 256, 128, 64, 32, 16, and 2 neurons, respectively. A dropout rate of 0.5 is used to prevent overfitting, with an additional dropout rate of 0.001 applied. The Xavier initialization function is used for weight initialization to enhance training convergence, and a fixed seed value of 12345L ensures reproducibility. The model includes 5 hidden layers, utilizes both Adam and SGD optimizers, and applies L2 regularization with a coefficient of 0.001. Training is conducted over 100 iterations with a learning rate of 0.1. Activation functions include ReLU, Tanh, and Softmax, and a momentum of 0.9 is used in the optimization process.
